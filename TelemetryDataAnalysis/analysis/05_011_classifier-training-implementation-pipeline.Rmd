---
title: "Classifier training and implementation pipeline"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", dpi = 500)

suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(depmixS4))

suppressPackageStartupMessages(library(DiagrammeR))
suppressPackageStartupMessages(library(DiagrammeRsvg))
suppressPackageStartupMessages(library(rsvg))

suppressPackageStartupMessages(library(kableExtra))

source(here::here("start.R"))
theme_set(theme_minimal())
set.seed(0)
```

## Goal

The goal of this notebook is to construct and refine a pipeline for training and implementing a classifier that detects the states of a push-up.

## Pipeline

The pipeline was first explored in the notebook [*Hidden Markov Model-based learning pipelines*](05_008_hmm_pipelines.md).
The following pipeline is based on this previous work, though refined to better suit a real use-case

### Steps

1. Chop the raw data within an IQR of the time steps to get just the clean push-up data.
2. Use an HMM to identify the 2 states of the push-up.
3. Use the HMM to cut the chopped data into the 3 states of a push-up (one state as `unknown`).
4. Train a classifier on this training data.
5. Apply the classifier to new data as it is being collected.

```{r, echo=FALSE, fig.height=5}
g <- grViz("
digraph a_nice_graph {

# node definitions with substituted label text
node [fontname = Helvetica, shape = 'oval']
a [label = 'example data', color = 'steelblue', penwidth = 2]
b [label = 'prepared example data']
c [label = 'Hidden Markov Model', shape = 'rectangle', penwidth = 2]
d [label = 'labeled example data']
e [label = 'Random Forest', shape = 'rectangle', penwidth = 2]
f [label = 'new data', color = 'mediumvioletred', penwidth = 2]
g [label = 'prepared new data']

# edge definitions with the node IDs
edge [fontname = Helvetica, fontcolor = 'grey50', penwidth = 1.5]
a->b [color = 'steelblue', label = ' scale & smooth']
b->c [color = 'steelblue', label = ' fit']
c->d [color = 'steelblue', label = ' predict']
d->e [color = 'steelblue', label = ' train']
f->g [color = 'mediumvioletred', label = ' smooth']
g->e [color = 'mediumvioletred', label = ' predict']
}
")


fn <- here(
  file.path(
    "analysis",
    "05_011_classifier-training-implementation-pipeline_files",
    "pipeline-diagram.png"
  )
)

export_svg(g) %>%
  charToRaw() %>%
  rsvg_png(file = fn, width = 1200, height = 1200)
```

<img src="05_011_classifier-training-implementation-pipeline_files/pipeline-diagram.png" width="600px">


### API

The key advancement of this notebook for this project needs to be the construction of a pipeline that can operate under real-world circumstances.
Thus, the API is important because, though the model will not be implemented in R, it will define the modularity of the pipeline.

---

## Data

```{r, warning=FALSE}
pushup_data <- tibble(filename = get_data_file_names(data_dir)) %.% {
  filter(str_detect(filename, "Push"))
  mutate(
    workout_idx = row_number(),
    all_data = map(filename, read_watch_data),
    file_info = map(all_data, ~ .x$meta_data),
    data = map(all_data, ~ .x$telemetry_data)
  )
}

train_dataset <- pushup_data$data[[1]]
new_dataset <- pushup_data$data[[6]]
```


## Pipeline

### 1. Scale and smooth the data for the HMM

```{r}
zscale <- function(x, na.rm = TRUE) {
  (x - mean(x, na.rm = na.rm)) / sd(x, na.rm = na.rm)
}


apply_scale_trans <- function(df, x = value, y = scaled_value) {
  df %.% {
    group_by(axis, motion)
    mutate({{ y }} := zscale({{ x }}))
    ungroup()
  }
}


apply_smoothing_trans <- function(df,
                                  x = value,
                                  y = smooth_value,
                                  rolling_n = 10) {
  before_after <- round(rolling_n / 2)

  df %.% {
    group_by(axis, motion)
    mutate(
      {{ y }} := slider::slide_dbl(
        {{ x }},
        .f = ~ max(abs(.x)),
        .before = before_after,
        .after = before_after
      ),
      {{ y }} := slider::slide_dbl(
        {{ y }},
        .f = mean,
        .before = 2,
        .after = 2
      )
    )
    ungroup()
  }
}


transform_pushup_data <- function(df) {
  df %>%
    apply_scale_trans(
      x = value,
      y = scaled_value
    ) %>%
    apply_smoothing_trans(
      x = value,
      y = smooth_value
    ) %>%
    apply_smoothing_trans(
      x = scaled_value,
      y = scaled_smooth_value
    )
}
```

```{r}
train_dataset <- transform_pushup_data(train_dataset)
train_dataset
```


### 2. Fit HMM

```{r}
pivot_telemetry_data <- function(telemetry_data, x = value) {
  telemetry_data %>%
    pivot_wider(
      c(date, idx),
      names_from = axis,
      values_from = {{ x }}
    )
}


prepare_hmm_fitting_data <- function(df, lower_q = 0.3, upper_q = 0.7) {
  df %>%
    filter(date > quantile(date, lower_q) & date < quantile(date, upper_q)) %>%
    pivot_telemetry_data(x = scaled_smooth_value)
}


construct_pushup_hmm <- function(d, nstates = 2) {
  depmix(
    list(
      x ~ 1,
      y ~ 1,
      z ~ 1,
      pitch ~ 1,
      roll ~ 1,
      yaw ~ 1
    ),
    data = d,
    nstates = nstates,
    family = list(
      gaussian(), gaussian(), gaussian(),
      gaussian(), gaussian(), gaussian()
    )
  )
}
```


```{r}
hmm_fit_data <- prepare_hmm_fitting_data(train_dataset)
pushup_hmm <- construct_pushup_hmm(hmm_fit_data)
pushup_hmm <- fit(pushup_hmm)
```
```{r}
train_dataset %>%
  filter(date %in% hmm_fit_data$date) %>%
  plot_hmm_fit(pushup_hmm, data_x = scaled_smooth_value)
```


### 3. Chop data into states

```{r}
prepare_rf_training_data <- function(hmm,
                                     hmm_data,
                                     full_data,
                                     prob_cutoff = 0.9,
                                     outer_unknown_q = 0.1) {
  hmm_posterior <- bind_cols(
    hmm_data,
    posterior(hmm)
  ) %.% {
    slice(-1) # Drop the first data point (often unreliable).
    mutate(state = case_when(
      S1 > prob_cutoff ~ "state1",
      S2 > prob_cutoff ~ "state2",
      TRUE ~ "unknown"
    ))
  }


  unknown_data <- full_data %.% {
    filter(
      date < quantile(date, outer_unknown_q) |
        date > quantile(date, 1 - outer_unknown_q)
    )
    add_column(state = "unknown")
  }

  d <- bind_rows(hmm_posterior, unknown_data)
  d <- d[sample(nrow(d)), ] # shuffle rows
  return(d)
}
```

```{r}
rf_training_data <- prepare_rf_training_data(
  hmm = pushup_hmm,
  hmm_data = train_dataset %>%
    filter(date %in% hmm_fit_data$date) %>%
    pivot_telemetry_data(x = smooth_value),
  full_data = pivot_telemetry_data(train_dataset),
)

table(rf_training_data$state)
```


### 4. Train Random Forest Classifier

```{r}
run_rf_workflow <- function(data, mtry = 3, trees = 100) {
  rf_spec <- rand_forest(
    mode = "classification",
    mtry = mtry,
    trees = trees,
    min_n = 50
  ) %>%
    set_engine("ranger")
  run_classifier_workflow(data, rf_spec)
}
```

```{r}
pushup_rf_res <- run_rf_workflow(rf_training_data, mtry = 1, trees = 16)
pushup_rf <- pushup_rf_res$fit_model[[1]]
```

```{r}
pushup_rf_res %.%
  {
    select(
      -fit_model, -train_pred_prob, -train_roc_curve,
      -test_pred_prob, -test_roc_curve, -train_pred_class,
      -test_pred_class
    )
    pivot_longer(-c())
    mutate(
      train_test = ifelse(str_detect(name, "train"), "train", "test"),
      name = str_remove(name, "train_|test_"),
      name = str_replace(name, "_", "-")
    )
    pivot_wider(train_test, names_from = name, values_from = value)
    rename(
      `train/test` = train_test
    )
  } %>%
  knitr::kable()
```


### 5. Smooth new data to be classified

```{r}
new_dataset <- apply_smoothing_trans(new_dataset, x = value)
plot_telmetry_data(df = new_dataset, x = smooth_value)
```


### 6. Make predictions on new data

```{r}
make_rf_prediction <- function(rf_mdl, df, type = "prob") {
  wide_df <- pivot_telemetry_data(df, x = smooth_value)

  pred_prob <- predict(
    rf_mdl,
    wide_df,
    type = "prob"
  )

  classes <- str_remove(colnames(pred_prob), "\\.pred_")
  pred_class <- classes[apply(pred_prob, 1, function(x) {
    which.max(x)
  })]


  bind_cols(wide_df, pred_prob) %>%
    mutate(.pred_class = pred_class)
}
```

```{r}
rf_new_pred <- make_rf_prediction(pushup_rf, new_dataset)
```

```{r}
most_common <- function(x) {
  names(sort(table(x), decreasing = TRUE))[[1]]
}

rf_new_pred <- rf_new_pred %>%
  mutate(sliding_pred = slider::slide_chr(
    .pred_class,
    most_common,
    .before = 5,
    .after = 5
  ))
```

```{r}
telemetry_plot <- plot_telmetry_data(new_dataset, x = smooth_value) +
  labs(x = NULL)

pushup_state_pal <- c(
  "state1" = "tomato",
  "state2" = "dodgerblue",
  "unknown" = "grey50"
)

pred_plot <- rf_new_pred %.%
  {
    select(idx, .pred_state1, .pred_state2, .pred_unknown)
    pivot_longer(-idx, names_to = "state", values_to = "prob")
    mutate(state = str_remove(state, "\\.pred_"))
    group_by(state)
    mutate(
      smooth_prob = slider::slide_dbl(
        prob,
        mean,
        .before = 20,
        .after = 20
      )
    )
  } %>%
  ggplot(aes(idx, color = state)) +
  geom_line(aes(y = prob), alpha = 1, size = 0.3) +
  geom_line(aes(y = smooth_prob), alpha = 0.9, size = 1) +
  scale_color_manual(values = pushup_state_pal) +
  labs(
    y = "probability",
    color = NULL,
    x = NULL,
    title = "Random Forest probabilities"
  )

pred_bar_plot <- rf_new_pred %>%
  ggplot(aes(idx, y = 1, color = sliding_pred)) +
  geom_line(aes(group = "a"), size = 10, alpha = 1) +
  scale_y_continuous(expand = expansion(mult = c(0, 0))) +
  scale_color_manual(values = pushup_state_pal) +
  theme(
    axis.text.y = element_blank(),
    legend.position = "none",
    panel.grid.major.y = element_blank()
  ) +
  labs(y = NULL)


(telemetry_plot / pred_plot / pred_bar_plot) +
  plot_layout(heights = c(4, 4, 1))
```
