---
title: "Hyperparameter tuning of the Random Forest Classifier"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", dpi = 500)

suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(depmixS4))

suppressPackageStartupMessages(library(kableExtra))

source(here::here("start.R"))
theme_set(theme_minimal())
set.seed(0)
```

The purpose of this notebook is the comprehensively tune the hyperparameters of a Random Forest (RF) Classifier for detecting states of a push-up.
The [*Hidden Markov Model-based learning pipelines*](05_008_hmm_pipelines.md) notebook demonstrated that a RF is likely the best model for this task, though did not pursue full optimization of the classifier.

At the end, an XGBoost model is also trained and tuned as a potential replacement for the RF.

Two blog-posts from Dr. Julia Silge that were useful guides for hyperparameter turning with the ['TidyModels']() library: [*Tuning random forest hyperparameters with #TidyTuesday trees data*](https://juliasilge.com/blog/sf-trees-random-tuning/) and [*Tune XGBoost with tidymodels and #TidyTuesday beach volleyball*](https://juliasilge.com/blog/xgboost-tune-volleyball/).


## Data

The data used here was cached in [*Hidden Markov Model-based learning pipelines*](05_008_hmm_pipelines.md).

```{r}
processed_pushup_data <- readRDS(
  here(file.path("cache", "hmm_processed_pushup_data.rds"))
)
processed_pushup_data
```

```{r}
# Data set of tuning model.
pushup_data <- processed_pushup_data$classifier_data[[1]]

# Training and testing split.
pushup_split <- initial_split(pushup_data, strata = state)
pushup_train <- training(pushup_split)
pushup_test <- testing(pushup_split)
```

## TidyModels workflow

### Recipe

```{r}
pushup_spec <- recipe(
  state ~ time_step + idx + x + y + z + pitch + yaw + roll,
  data = pushup_train
) %>%
  update_role(idx, time_step, new_role = "ID")

pushup_spec
```

```{r}
pushup_spec$var_info
```


### Model Specification

```{r}
tune_spec <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine(
    "ranger",
    max.depth = tune()
  )

tune_spec
```


### Cross Validation data split

```{r}
pushup_cvfolds <- vfold_cv(pushup_train, v = 10)
pushup_cvfolds
```

### Workflow

```{r}
pushup_tune_wf <- workflow() %>%
  add_recipe(pushup_spec) %>%
  add_model(tune_spec)

pushup_tune_wf
```


### Hyperparameter grid search

#### Coarse random grid search

```{r}
# Parameters for coarse tuning.
rf_params_coarse <- pushup_tune_wf %>%
  parameters() %>%
  update(
    mtry = mtry(range = c(1L, 6L)),
    trees = trees(range = c(100L, 2000L)),
    min_n = min_n(),
    max.depth = tree_depth(range = c(1, 200))
  )

# Random coarse tuning grid.
set.seed(0)
coarse_rand_grid <- grid_random(rf_params_coarse, size = 100)
GGally::ggpairs(coarse_rand_grid, progress = FALSE)

# Metrics to collect.
tuning_metric_set <- metric_set(
  sensitivity, specificity, accuracy, roc_auc, ppv
)

# Register cores for parallel processing.
doParallel::registerDoParallel()

stash(
  "rf_tune_coarse",
  depends_on = c("coarse_rand_grid", "tuning_metric_set"),
  {
    rf_tune_coarse <- tune_grid(
      pushup_tune_wf,
      resamples = pushup_cvfolds,
      grid = coarse_rand_grid,
      metrics = tuning_metric_set
    )
  }
)
```

```{r}
# Plot the results of a `metric` for all of the tuning hyperparameters.
rf_param_tuning_plot <- function(tune_res,
                                 metric,
                                 param1 = mtry,
                                 param2 = max.depth) {
  tune_res %>%
    collect_metrics() %>%
    filter(.metric == !!metric) %>%
    select(mean, {{ param1 }}:{{ param2 }}) %>%
    pivot_longer(
      {{ param1 }}:{{ param2 }},
      values_to = "value",
      names_to = "parameter"
    ) %>%
    ggplot(aes(value, mean, color = parameter)) +
    facet_wrap(~parameter, scales = "free_x") +
    geom_point(show.legend = FALSE) +
    geom_smooth(method = "loess", formula = "y ~ x") +
    scale_color_brewer(type = "qual", palette = "Set1") +
    labs(
      x = NULL,
      y = metric
    )
}
```

```{r}
tibble(
  metric = c("accuracy", "roc_auc", "sens", "spec", "ppv"),
  y_min = c(0.96, 0.996, 0.96, 0.98, 0.95)
) %>%
  pwalk(function(metric, y_min) {
    p <- rf_param_tuning_plot(rf_tune_coarse, metric) +
      scale_y_continuous(
        limits = c(y_min, 1),
        expand = expansion(mult = c(0, 0))
      )
    plot(p)
  })
```

```{r}
collect_metrics(rf_tune_coarse) %.%
  {
    group_by(.metric)
    top_n(n = 5, wt = mean)
    ungroup()
    arrange(.metric, mean, -mtry - trees - min_n - max.depth)
    mutate(idx = row_number())
    select(idx, mtry:.metric)
    pivot_longer(-c(.metric, idx))
  } %>%
  ggplot(aes(x = .metric, y = value)) +
  facet_wrap(~name, scales = "free_y") +
  geom_boxplot(aes(color = .metric, fill = .metric), outlier.shape = NA, alpha = 0.1) +
  geom_jitter(aes(color = .metric), height = 0, width = 0.2, alpha = 0.8)
```


#### Fine random grid search

```{r}
# Parameters for coarse tuning.
rf_params_fine <- pushup_tune_wf %>%
  parameters() %>%
  update(
    mtry = mtry(range = c(2L, 3L)),
    trees = trees(range = c(1000L, 2000L)),
    min_n = min_n(range = c(3, 20)),
    max.depth = tree_depth(range = c(2, 10))
  )

# Random fine tuning grid.
set.seed(0)
fine_rand_grid <- grid_random(rf_params_fine, size = 100)

stash(
  "rf_tune_fine",
  depends_on = c("fine_rand_grid", "tuning_metric_set"),
  {
    rf_tune_fine <- tune_grid(
      pushup_tune_wf,
      resamples = pushup_cvfolds,
      grid = fine_rand_grid,
      metrics = tuning_metric_set
    )
  }
)
```

```{r}
tibble(
  metric = c("accuracy", "roc_auc", "sens", "spec", "ppv"),
  y_min = c(0.96, 0.996, 0.96, 0.98, 0.95)
) %>%
  pwalk(function(metric, y_min) {
    p <- rf_param_tuning_plot(rf_tune_fine, metric) +
      scale_y_continuous(
        limits = c(y_min, 1),
        expand = expansion(mult = c(0, 0))
      )
    plot(p)
  })
```

```{r}
metrics <- unique(collect_metrics(rf_tune_fine)$.metric)[1:3]
map_dfr(metrics, ~ show_best(rf_tune_fine, metric = .x)) %>%
  knitr::kable(format = "markdown")
```

### Optimal hyperparameters

| **hyperparameter** | **value** |
|--------------------|-----------|
| mtry               | 2         |
| trees              | 1200      |
| min_n              | 9         |
| max.depth          | 8         |

---

## XGBoost

If a RF classifier performs well, often a XGBoost can increased performance.
Therefore, some exploration of this drop-in replacement is conducted below.

### Tuning hyperparameters

```{r}
xgb_spec <- boost_tree(
  mtry = tune(),
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgb_spec
```

```{r}
pushup_xgb_tune_wf <- workflow() %>%
  add_recipe(pushup_spec) %>%
  add_model(xgb_spec)
```

```{r}
set.seed(0)
xgb_coarse_grid <- grid_random(
  mtry(range = c(1L, 6L)),
  trees(range = c(50, 2000)),
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  sample_size = sample_prop(),
  size = 100
)

stash(
  "xgb_tune_coarse",
  depends_on = c("xgb_coarse_grid", "tuning_metric_set"),
  {
    xgb_tune_coarse <- tune_grid(
      pushup_xgb_tune_wf,
      resamples = pushup_cvfolds,
      grid = xgb_coarse_grid,
      metrics = tuning_metric_set
    )
  }
)
```

```{r, warning=FALSE}
tibble(
  metric = c("accuracy", "roc_auc", "sens", "spec", "ppv"),
  y_min = c(0.95, 0.95, 0.95, 0.95, 0.95)
) %>%
  pwalk(function(metric, y_min) {
    p <- rf_param_tuning_plot(
      xgb_tune_coarse,
      metric,
      param1 = mtry,
      param2 = loss_reduction
    ) +
      scale_y_continuous(
        limits = c(y_min, 1),
        expand = expansion(mult = c(0, 0))
      )
    plot(p)
  })
```

### Conclusion

It does not seem like there is improvement from using an XGBoost model, likely because the RF model performed as well as could  be hoped for.
There are drawbacks to using a boosted model (e.g. non-parallel training), so I will continue with the RF.
